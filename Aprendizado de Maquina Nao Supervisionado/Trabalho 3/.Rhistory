set.seed(42)
nTreeList = c(1, 5, 10, 25, 50, 100, 250, 500, 1000)
accPerNTrees <- data.frame(ntree=numeric(length(nTreeList)),
accTrain=numeric(length(nTreeList)),
accVal=numeric(length(nTreeList)))
for (i in 1:length(nTreeList)){
rfModel <- randomForest(formula=class ~ variance + skewness
+ curtosis + entropy,
data= trainSet, ntree=nTreeList[i], mtry=3)
# Avaliando no conjunto de treinamento
train_pred <- predict(rfModel, trainSet, type="class")
cm_train <- confusionMatrix(data = as.factor(train_pred),
reference = as.factor(trainSet$class),
positive='forgery')
cm_train_relative <- calculaMatrizConfusaoRelativa(cm_train)
acc_bal_train <- (cm_train_relative[1,1] + cm_train_relative[2,2])/2
# Avaliando no conjunto de validacao
val_pred <- predict(rfModel, valSet, type="class")
cm_val <- confusionMatrix(data = as.factor(val_pred),
reference = as.factor(valSet$class),
positive='forgery')
cm_val_relative <- calculaMatrizConfusaoRelativa(cm_val)
acc_bal_val <- (cm_val_relative[1,1] + cm_val_relative[2,2])/2
accPerNTrees[i,] = c(nTreeList[i],
acc_bal_train,
acc_bal_val)
}
accPerNTrees <- melt(accPerNTrees, id="ntree")  # convert to long format
ggplot(data=accPerNTrees, aes(x=ntree, y=value, colour=variable)) + geom_line() + geom_point()
#### Avalia??o da melhor floresta no teste ####
# Treina a Floresta Aleat?ria
rfModel <- randomForest(formula=class ~ variance + skewness
+ curtosis + entropy,
data= trainSet, ntree=100)
test_pred <- predict(rfModel, testSet, type="class")
cm_test <- confusionMatrix(data = as.factor(test_pred),
reference = as.factor(testSet$class),
positive='forgery')
cm_test_relative <- calculaMatrizConfusaoRelativa(cm_test)
cm_test_relative
acc_bal_test <- (cm_test_relative[1,1] + cm_test_relative[2,2])/2
acc_bal_test
######## Execu??o do Exemplo da Aula #############
classExample <- data.frame(tempo=c("ensolarado", "ensolarado", "nublado","chover",
"chover", "chover", "nublado", "ensolarado",
"ensolarado", "chover", "ensolarado", "nublado",
"nublado", "chover"),
temperatura=c("calor", "calor", "calor", "moderada", "frio",
"frio", "frio", "moderada", "frio", "moderada",
"moderada", "moderada", "calor", "moderada"),
umidade=c("Alta", "Alta", "Alta", "Alta", "Normal", "Normal",
"Normal", "Alta", "Normal", "Normal", "Normal",
"Alta", "Normal", "Alta"),
vento=c("Fraco", "Forte", "Fraco", "Fraco", "Fraco", "Forte",
"Forte", "Fraco", "Fraco", "Fraco", "Forte", "Forte",
"Fraco", "Forte"),
target=c("nao", "nao", "sim", "sim", "sim", "nao", "sim",
"nao", "sim", "sim", "sim", "sim", "sim", "nao"),
stringsAsFactors = TRUE)
dim(classExample)
summary(classExample)
treeModel <- rpart(formula=target ~ tempo + temperatura + umidade + vento,
data=classExample, method="class",
control=rpart.control(minsplit=2, cp=0.0, xval = 0),
parms= list(split="information"))
printcp(treeModel)
summary(treeModel)
# Plota a ?rvore de decis?o podada
rpart.plot(treeModel,
extra=104, box.palette="GnBu",
branch.lty=3, shadow.col="gray", nn=TRUE)
# Plota a ?rvore de decis?o podada
rpart.plot(treeModel,
extra=104, box.palette="GnBu",
branch.lty=3, shadow.col="gray", nn=TRUE)
Entropy_S = -((5/14)*log(5/14) + (9/14)*log(9/14))
Entropy_S
Entropy_chover_ensolarado = -((5/10)*log(5/10) + (5/10)*log(5/10))
Entropy_chover_ensolarado
# Zero substituido por 1e-12 para n?o resultar em NaN
Entropy_nublado = -((0/4)*log(1e-12/4) + (4/4)*log(4/4))
Entropy_nublado
gain = Entropy_S - (10/14)*Entropy_chover_ensolarado - (4/14)*Entropy_nublado
gain
improve = 14*gain
improve
rm(list=ls())
graphics.off()
knitr::opts_chunk$set(echo = TRUE, error = FALSE, message = FALSE, warning = FALSE, tidy = FALSE)
options(digits = 3)
setwd("~/GitHub/mineiracao_dados_complexos/Aprendizado de Maquina Nao Supervisionado/Trabalho 3")
# Adicione os pacotes usados neste trabalho:
#install packages
#install.packages("apcluster")
#install.packages("fclust")
#install.packages("ppclust")
#install.packages("dbscan")
#install.packages("gridExtra")
#install.packages("fpc")
#install.packages("NbClust")
#install.packages("factoextra")
#install.packages("NbClust")
library(dplyr)
library(ggplot2)
library(factoextra)
library(NbClust)
library(fpc)
library(gridExtra)
library(dbscan)
library(ppclust)
library(fclust)
library(apcluster)
library ( NbClust )
#Funções extras
onehot_features <- function(dataset, feature){
cats <- unique(dataset[, feature])
for (cat in cats){
dataset[cat] <- as.numeric(dataset[,feature]==cat)
}
return(dataset)
}
min_max <- function(df, columns){
min_features <- apply(df[,colnames(df) %in% columns], 2, min); min_features
max_features <- apply(df[,colnames(df) %in% columns], 2, max); max_features
diff <- max_features - min_features; diff
df[,colnames(df) %in% columns] <- sweep(df[,colnames(df) %in% columns], 2, min_features, "-")
df[,colnames(df) %in% columns] <- sweep(df[,colnames(df) %in% columns], 2, diff, "/")
return(df)
}
# Configure ambiente de trabalho na mesma pasta
# onde colocou a base de dados:
# setwd("")
# setwd("C:\\Users\\Eric\\Documents\\GitHub\\mineiracao_dados_complexos\\Aprendizado de Maquina Nao Supervisionado\\Trabalho 3")
# setwd("/Users/nkuros/Documents/mineiracao_dados_complexos/Aprendizado de Maquina Nao Supervisionado/Trabalho 3/")
# Leitura da base
df <- read.table('imports-85.data',sep=',')
set.seed (12345)
# Tratamento de dados faltantes
# Avaliacao do dataset
summary(df)
head(df)
# Avalaiacao dos valores unicos de cada coluna
for(col in colnames(df)){
print(col)
print(unique(df[,col]))
print('\n')
}
#------------------------------------------------------------------------
# Tratando "?" que apresenta nas bases para poder transformar em numerico
df$V2 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V2))
df$V6 <- gsub(pattern='[?]', replacement=NA, df$V6)
df$V19 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V19))
df$V20 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V20))
df$V22 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V22))
df$V23 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V23))
df$V26 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V26))
unique(df$V6)
unique(df$V18)
#----------------------------------------
# Data set de teste retirando todos os NA
df2 <- na.omit(df)
summary(df2)
#----------------------------------------
# NA de V19 e V20 sao para a mesma marca mazda
df[is.na(df$V19),]
df[df$V3=='mazda',]
# Agrupando os valores para obter a m[edia de V19 e V20]
linhas = df$V3=='mazda' & !is.na(df$V19)
colunas = c('V3', 'V19', 'V20')
media_mazda = df[linhas,colunas] %>% group_by(V3) %>% summarise(media_v19=median(V19), media_V20=median(V20))
# Substituindo V19 e V20 que estao NA pela media
df[df$V3=='mazda' & is.na(df$V19),'V19'] = media_mazda$media_v19
df[df$V3=='mazda' & is.na(df$V20),'V20'] = media_mazda$media_V20
#----------------------------------------------------------------------------------------
# Como temos apenas 2 carros da marca renault, vamos observar a distribuicao de V22 e V23
df[is.na(df$V22),]
df[df$V3=='renault',]
# Distribuicoes nao normais e outliers, vamos pela mediana
ggplot(df, aes(x=V22, y= ..density..)) +
geom_histogram(color='White', bins=10) +
geom_density()
ggplot(df, aes(x=V23, y= ..density..)) +
geom_histogram(color='White', bins=10) +
geom_density()
linhas <- !is.na(df$V22)
colunas <- c('V22', 'V23')
media_renault <- df[linhas,colunas] %>% summarise(mediana_v22=median(V22), mediana_V23=median(V23))
df[df$V3=='renault' & is.na(df$V22),'V22'] = media_renault$mediana_v22
df[df$V3=='renault' & is.na(df$V23),'V23'] = media_renault$mediana_V23
#----------------------------------------
# Verificacao dos veiculos com preco NA
df[is.na(df$V26),]
# Marcas para verificar 'audi', 'isuzu', 'porsche'
df[df$V3 %in% c('audi', 'isuzu', 'porsche'),]
#df[df$V3 %in% c('audi', 'isuzu', 'porsche') & !is.na(df$V26), c('V3', 'V26')]
#    %>% group_by(V3)
#    %>% summarise(max=max(V26), min=min(V26), mean=mean(V26), median=median(V26))
linhas <- df$V3 %in% c('audi', 'isuzu', 'porsche') & !is.na(df$V26)
colunas <- c('V3', 'V26')
median_marcas <- df[linhas, colunas] %>% group_by(V3) %>% summarise(median_26=median(V26))
for (marca in unique(median_marcas$V3)){
df[df$V3==marca & is.na(df$V26),'V26'] <- median_marcas[median_marcas$V3==marca,'median_26']
}
#----------------------
# Verificando coluna V2
teste <- df[df$V3 %in% unique(df[is.na(df$V2),'V3']),]
teste
# Para as marcas que possuem mais carros com V2 nao nulo, tiramos a media
linhas <- df$V3 %in% unique(df[is.na(df$V2),'V3']) & !is.na(df$V2)
colunas <- c('V3', 'V2')
media_marcas <- df[linhas, colunas] %>% group_by(V3) %>% summarise(median_2=median(V2))
media_marcas
for (marca in unique(media_marcas$V3) ){
df[df$V3==marca & is.na(df$V2),'V2'] <- media_marcas[media_marcas$V3==marca,'median_2']
}
# Para as marcas em que todos os carros apresentam V2 nulo, tiramos a mediana da base
ggplot(df, aes(x=V2, y= ..density..)) +
geom_histogram(color='White', bins=10) +
geom_density()
linhas <- !is.na(df$V2)
colunas <- 'V2'
mediana_marcas <- median(df[linhas, colunas])
mediana_marcas
for (marca in unique(df[is.na(df$V2), 'V3'])) {
df[df$V3==marca & is.na(df$V2),'V2'] <- mediana_marcas
}
# retirando duas linhas
# df <- na.omit(df, cols='V6')
# Criando features Categoricas via One Hot Encoding
df = onehot_features(df, 'V3')
df = onehot_features(df, 'V4')
df = onehot_features(df, 'V5')
#df = onehot_features(df, 'V6')
df = onehot_features(df, 'V7')
df = onehot_features(df, 'V8')
df = onehot_features(df, 'V9')
df = onehot_features(df, 'V15')
df = onehot_features(df, 'V16')
df = onehot_features(df, 'V18')
# NORMALIZAR AS VARIAVEIS
summary(df)
# Construindo um gráfico com as distâncias intra-cluster
#colunas_porta = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "four")
#colunas_tracao = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "rwd", "fwd")
#colunas_cilindro = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "four", "six", "five", "three", "twelve", "two")
colunas_full = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "convertible", "hatchback", "sedan", "wagon", "four", "six", "five", "three", "twelve", "two")
#colunas = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "convertible", "hatchback", "sedan", "wagon")
colunas = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26")
numericas = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26")
df_normalized <- min_max(df,numericas)
summary(df_normalized)
colnames(df_normalized)
##### ALGORITIMO PARA TAMANHO DE CLUSTERS ####
base_1 = df_normalized[, colunas_full]
nb <- NbClust ( base_1 , distance ="euclidean",min.nc =2 , max.nc =30 , method ="complete",index ="all")
##### ALGORITIMO PARA TAMANHO DE CLUSTERS ####
base_1 = df_normalized[, numericas]
nb <- NbClust ( base_1 , distance ="euclidean",min.nc =2 , max.nc =30 , method ="complete",index ="all")
# Validacao do conteudo de cada coluna para ver se tratamos todos os casos de [?]
for(col in colnames(df)){
print(col)
print(unique(df[,col]))
print('----------------------------')
}
nb <- NbClust ( base_1 , distance ="euclidean",min.nc =2 , max.nc =30 , method ="complete",index ="all")
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE, error = FALSE, message = FALSE, warning = FALSE, tidy = FALSE)
options(digits = 3)
# Adicione os pacotes usados neste trabalho:
#install packages
#install.packages("apcluster")
#install.packages("fclust")
#install.packages("ppclust")
#install.packages("dbscan")
#install.packages("gridExtra")
#install.packages("fpc")
#install.packages("NbClust")
#install.packages("factoextra")
#install.packages("NbClust")
library(dplyr)
library(ggplot2)
library(factoextra)
library(NbClust)
library(fpc)
library(gridExtra)
library(dbscan)
library(ppclust)
library(fclust)
library(apcluster)
library ( NbClust )
#Funções extras
onehot_features <- function(dataset, feature){
cats <- unique(dataset[, feature])
for (cat in cats){
dataset[cat] <- as.numeric(dataset[,feature]==cat)
}
return(dataset)
}
min_max <- function(df, columns){
min_features <- apply(df[,colnames(df) %in% columns], 2, min); min_features
max_features <- apply(df[,colnames(df) %in% columns], 2, max); max_features
diff <- max_features - min_features; diff
df[,colnames(df) %in% columns] <- sweep(df[,colnames(df) %in% columns], 2, min_features, "-")
df[,colnames(df) %in% columns] <- sweep(df[,colnames(df) %in% columns], 2, diff, "/")
return(df)
}
# Configure ambiente de trabalho na mesma pasta
# onde colocou a base de dados:
# setwd("")
# setwd("C:\\Users\\Eric\\Documents\\GitHub\\mineiracao_dados_complexos\\Aprendizado de Maquina Nao Supervisionado\\Trabalho 3")
setwd("/Users/nkuros/Documents/mineiracao_dados_complexos/Aprendizado de Maquina Nao Supervisionado/Trabalho 3/")
setwd("~/GitHub/mineiracao_dados_complexos/Aprendizado de Maquina Nao Supervisionado/Trabalho 3")
# Leitura da base
df <- read.table('imports-85.data',sep=',')
set.seed (12345)
# Tratamento de dados faltantes
# Avaliacao do dataset
summary(df)
head(df)
# Avalaiacao dos valores unicos de cada coluna
for(col in colnames(df)){
print(col)
print(unique(df[,col]))
print('\n')
}
#------------------------------------------------------------------------
# Tratando "?" que apresenta nas bases para poder transformar em numerico
df$V2 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V2))
df$V6 <- gsub(pattern='[?]', replacement=NA, df$V6)
df$V19 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V19))
df$V20 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V20))
df$V22 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V22))
df$V23 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V23))
df$V26 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V26))
unique(df$V6)
unique(df$V18)
#----------------------------------------
# Data set de teste retirando todos os NA
df2 <- na.omit(df)
summary(df2)
#----------------------------------------
# NA de V19 e V20 sao para a mesma marca mazda
df[is.na(df$V19),]
df[df$V3=='mazda',]
# Agrupando os valores para obter a m[edia de V19 e V20]
linhas = df$V3=='mazda' & !is.na(df$V19)
colunas = c('V3', 'V19', 'V20')
media_mazda = df[linhas,colunas] %>% group_by(V3) %>% summarise(media_v19=median(V19), media_V20=median(V20))
# Substituindo V19 e V20 que estao NA pela media
df[df$V3=='mazda' & is.na(df$V19),'V19'] = media_mazda$media_v19
df[df$V3=='mazda' & is.na(df$V20),'V20'] = media_mazda$media_V20
#----------------------------------------------------------------------------------------
# Como temos apenas 2 carros da marca renault, vamos observar a distribuicao de V22 e V23
df[is.na(df$V22),]
df[df$V3=='renault',]
# Distribuicoes nao normais e outliers, vamos pela mediana
ggplot(df, aes(x=V22, y= ..density..)) +
geom_histogram(color='White', bins=10) +
geom_density()
ggplot(df, aes(x=V23, y= ..density..)) +
geom_histogram(color='White', bins=10) +
geom_density()
linhas <- !is.na(df$V22)
colunas <- c('V22', 'V23')
media_renault <- df[linhas,colunas] %>% summarise(mediana_v22=median(V22), mediana_V23=median(V23))
df[df$V3=='renault' & is.na(df$V22),'V22'] = media_renault$mediana_v22
df[df$V3=='renault' & is.na(df$V23),'V23'] = media_renault$mediana_V23
#----------------------------------------
# Verificacao dos veiculos com preco NA
df[is.na(df$V26),]
# Marcas para verificar 'audi', 'isuzu', 'porsche'
df[df$V3 %in% c('audi', 'isuzu', 'porsche'),]
#df[df$V3 %in% c('audi', 'isuzu', 'porsche') & !is.na(df$V26), c('V3', 'V26')]
#    %>% group_by(V3)
#    %>% summarise(max=max(V26), min=min(V26), mean=mean(V26), median=median(V26))
linhas <- df$V3 %in% c('audi', 'isuzu', 'porsche') & !is.na(df$V26)
colunas <- c('V3', 'V26')
median_marcas <- df[linhas, colunas] %>% group_by(V3) %>% summarise(median_26=median(V26))
for (marca in unique(median_marcas$V3)){
df[df$V3==marca & is.na(df$V26),'V26'] <- median_marcas[median_marcas$V3==marca,'median_26']
}
#----------------------
# Verificando coluna V2
teste <- df[df$V3 %in% unique(df[is.na(df$V2),'V3']),]
teste
# Para as marcas que possuem mais carros com V2 nao nulo, tiramos a media
linhas <- df$V3 %in% unique(df[is.na(df$V2),'V3']) & !is.na(df$V2)
colunas <- c('V3', 'V2')
media_marcas <- df[linhas, colunas] %>% group_by(V3) %>% summarise(median_2=median(V2))
media_marcas
for (marca in unique(media_marcas$V3) ){
df[df$V3==marca & is.na(df$V2),'V2'] <- media_marcas[media_marcas$V3==marca,'median_2']
}
# Para as marcas em que todos os carros apresentam V2 nulo, tiramos a mediana da base
ggplot(df, aes(x=V2, y= ..density..)) +
geom_histogram(color='White', bins=10) +
geom_density()
linhas <- !is.na(df$V2)
colunas <- 'V2'
mediana_marcas <- median(df[linhas, colunas])
mediana_marcas
for (marca in unique(df[is.na(df$V2), 'V3'])) {
df[df$V3==marca & is.na(df$V2),'V2'] <- mediana_marcas
}
# retirando duas linhas
# df <- na.omit(df, cols='V6')
# Validacao do conteudo de cada coluna para ver se tratamos todos os casos de [?]
for(col in colnames(df)){
print(col)
print(unique(df[,col]))
print('----------------------------')
}
# Construindo um gráfico com as distâncias intra-cluster
# colunas_porta = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "four")
# colunas_tracao = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "rwd", "fwd")
# colunas_cilindro = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "four", "six", "five", "three", "twelve", "two")
colunas_full = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "convertible", "hatchback", "sedan", "wagon", "four", "six", "five", "three", "twelve", "two")
#colunas = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "convertible", "hatchback", "sedan", "wagon")
colunas = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26")
numericas = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26")
# Normalizacao minmax das variaveis numericas
df_normalized <- min_max(df,numericas)
summary(df_normalized)
colnames(df_normalized)
# Seleção de atributos
# Iremos criar o onehot encoding para todas as features categoricas, assim podemos testar a sua eficiencia ao rodar os algoritmos de clusterizacao
# Criando features Categoricas via One Hot Encoding
df_normalized = onehot_features(df_normalized, 'V3')
df_normalized = onehot_features(df_normalized, 'V4')
df_normalized = onehot_features(df_normalized, 'V5')
# df_normalized = onehot_features(df_normalized, 'V6')
df_normalized = onehot_features(df_normalized, 'V7')
df_normalized = onehot_features(df_normalized, 'V8')
df_normalized = onehot_features(df_normalized, 'V9')
df_normalized = onehot_features(df_normalized, 'V15')
df_normalized = onehot_features(df_normalized, 'V16')
df_normalized = onehot_features(df_normalized, 'V18')
summary(df_normalized)
base_1 = df_normalized[, numericas]
fviz_nbclust(base_1, kmeans, method="wss",k.max=3)
fviz_nbclust(base_1, kmeans, method="wss",k.max=30)
# Aplica a tecnica K-Means
base_kmeans <- eclust(base_1, "kmeans", k=30,nstart=25, graph=TRUE)
fviz_nbclust(base_1, kmeans, method="wss",k.max=30)
# Construindo um gráfico com os valores da silhueta
fviz_nbclust(base_1, kmeans, method="silhouette", k.max=30)
melt_train_set <- df[, numericas]
melt_train_set <- melt(melt_train_set)
p <- ggplot(data=melt_train_set, aes(x=value))+
stat_density()+
facet_wrap(~variable, scales='free'); p
base_1 = df_normalized[, numericas_sem_21]
numericas_sem_21 = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V22", "V23", "V24", "V25", "V26")
base_1 = df_normalized[, numericas_sem_21]
fviz_nbclust(base_1, kmeans, method="wss",k.max=30)
base_1 = df_normalized[, numericas]
fviz_nbclust(base_1, kmeans, method="wss",k.max=30)
base_1 = df_normalized[, numericas_sem_21]
# Construindo um gráfico com os valores da silhueta
fviz_nbclust(base_1, kmeans, method="silhouette", k.max=30)
p <- ggplot(data=melt_train_set, aes(x=value))+
stat_density()+
facet_wrap(~variable, scales='free'); p
numericas_reduzido = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V23", "V24", "V25", "V26")
base_1 = df_normalized[, numericas_reduzido]
fviz_nbclust(base_1, kmeans, method="wss",k.max=30)
# Construindo um gráfico com os valores da silhueta
fviz_nbclust(base_1, kmeans, method="silhouette", k.max=30)
summary(base_1)
p <- ggplot(data=melt_train_set, aes(x=value))+
stat_density()+
facet_wrap(~variable, scales='free'); p
numericas_reduzido = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V20", "V23", "V24", "V25", "V26")
base_1 = df_normalized[, numericas_reduzido]
fviz_nbclust(base_1, kmeans, method="wss",k.max=30)
# Construindo um gráfico com os valores da silhueta
fviz_nbclust(base_1, kmeans, method="silhouette", k.max=30)
nb <- NbClust ( base_1 , distance ="euclidean",min.nc =2 , max.nc =30 , method ="complete",index ="all")
#TESTE KMEANS COM 3 para análise de grupos
base_kmeans_test <- kmeans(base_1 , 2, nstart = 25)
test_df <- as.data.frame.matrix(table(base_kmeans_test$cluster, df_normalized$V1));test_df
View(test_df)
p <- ggplot(data=melt_train_set, aes(x=value))+
stat_density()+
facet_wrap(~variable, scales='free'); p
numericas_reduzido = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V23", "V24", "V25", "V26")
fviz_nbclust(base_1, kmeans, method="wss",k.max=30)
base_1 = df_normalized[, numericas_reduzido]
fviz_nbclust(base_1, kmeans, method="wss",k.max=30)
# Construindo um gráfico com os valores da silhueta
fviz_nbclust(base_1, kmeans, method="silhouette", k.max=30)
# Aplica a tecnica K-Means
base_kmeans <- eclust(base_1, "kmeans", k=30,nstart=25, graph=TRUE)
# Aplica a tecnica K-Means
base_kmeans <- eclust(base_1, "kmeans", k=2,nstart=25, graph=TRUE)
#TESTE KMEANS COM 3 para análise de grupos
base_kmeans_test <- kmeans(base_1 , 2, nstart = 25)
test_df <- as.data.frame.matrix(table(base_kmeans_test$cluster, df_normalized$V1));test_df
#TESTE KMEANS COM 3 para análise de grupos
base_kmeans_test <- kmeans(base_1 , 4, nstart = 25)
test_df <- as.data.frame.matrix(table(base_kmeans_test$cluster, df_normalized$V1));test_df
p <- ggplot(data=melt_train_set, aes(x=value))+
stat_density()+
facet_wrap(~variable, scales='free'); p
numericas_reduzido = c("V2", "V10", "V11", "V12", "V13", "V14", "V19", "V23", "V24", "V25", "V26")
base_1 = df_normalized[, numericas_reduzido]
fviz_nbclust(base_1, kmeans, method="wss",k.max=30)
# Construindo um gráfico com os valores da silhueta
fviz_nbclust(base_1, kmeans, method="silhouette", k.max=30)
nb <- NbClust ( base_1 , distance ="euclidean",min.nc =2 , max.nc =30 , method ="complete",index ="all")
nb <- NbClust ( base_1 , distance ="manhattan",min.nc =2 , max.nc =30 , method ="complete",index ="all")
nb <- NbClust ( base_1 , distance ="euclidian",min.nc =2 , max.nc =30 , method ="complete",index ="all")
