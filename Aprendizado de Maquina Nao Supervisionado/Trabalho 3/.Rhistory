base_1 = df_normalized[, numericas]
nb <- NbClust ( base_1 , distance ="euclidean",min.nc =2 , max.nc =30 , method ="complete",index ="all")
# Validacao do conteudo de cada coluna para ver se tratamos todos os casos de [?]
for(col in colnames(df)){
print(col)
print(unique(df[,col]))
print('----------------------------')
}
nb <- NbClust ( base_1 , distance ="euclidean",min.nc =2 , max.nc =30 , method ="complete",index ="all")
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE, error = FALSE, message = FALSE, warning = FALSE, tidy = FALSE)
options(digits = 3)
# Adicione os pacotes usados neste trabalho:
#install packages
#install.packages("apcluster")
#install.packages("fclust")
#install.packages("ppclust")
#install.packages("dbscan")
#install.packages("gridExtra")
#install.packages("fpc")
#install.packages("NbClust")
#install.packages("factoextra")
#install.packages("NbClust")
library(dplyr)
library(ggplot2)
library(factoextra)
library(NbClust)
library(fpc)
library(gridExtra)
library(dbscan)
library(ppclust)
library(fclust)
library(apcluster)
library ( NbClust )
#Funções extras
onehot_features <- function(dataset, feature){
cats <- unique(dataset[, feature])
for (cat in cats){
dataset[cat] <- as.numeric(dataset[,feature]==cat)
}
return(dataset)
}
min_max <- function(df, columns){
min_features <- apply(df[,colnames(df) %in% columns], 2, min); min_features
max_features <- apply(df[,colnames(df) %in% columns], 2, max); max_features
diff <- max_features - min_features; diff
df[,colnames(df) %in% columns] <- sweep(df[,colnames(df) %in% columns], 2, min_features, "-")
df[,colnames(df) %in% columns] <- sweep(df[,colnames(df) %in% columns], 2, diff, "/")
return(df)
}
# Configure ambiente de trabalho na mesma pasta
# onde colocou a base de dados:
# setwd("")
# setwd("C:\\Users\\Eric\\Documents\\GitHub\\mineiracao_dados_complexos\\Aprendizado de Maquina Nao Supervisionado\\Trabalho 3")
setwd("/Users/nkuros/Documents/mineiracao_dados_complexos/Aprendizado de Maquina Nao Supervisionado/Trabalho 3/")
setwd("~/GitHub/mineiracao_dados_complexos/Aprendizado de Maquina Nao Supervisionado/Trabalho 3")
# Leitura da base
df <- read.table('imports-85.data',sep=',')
set.seed (12345)
# Tratamento de dados faltantes
# Avaliacao do dataset
summary(df)
head(df)
# Avalaiacao dos valores unicos de cada coluna
for(col in colnames(df)){
print(col)
print(unique(df[,col]))
print('\n')
}
#------------------------------------------------------------------------
# Tratando "?" que apresenta nas bases para poder transformar em numerico
df$V2 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V2))
df$V6 <- gsub(pattern='[?]', replacement=NA, df$V6)
df$V19 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V19))
df$V20 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V20))
df$V22 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V22))
df$V23 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V23))
df$V26 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V26))
unique(df$V6)
unique(df$V18)
#----------------------------------------
# Data set de teste retirando todos os NA
df2 <- na.omit(df)
summary(df2)
#----------------------------------------
# NA de V19 e V20 sao para a mesma marca mazda
df[is.na(df$V19),]
df[df$V3=='mazda',]
# Agrupando os valores para obter a m[edia de V19 e V20]
linhas = df$V3=='mazda' & !is.na(df$V19)
colunas = c('V3', 'V19', 'V20')
media_mazda = df[linhas,colunas] %>% group_by(V3) %>% summarise(media_v19=median(V19), media_V20=median(V20))
# Substituindo V19 e V20 que estao NA pela media
df[df$V3=='mazda' & is.na(df$V19),'V19'] = media_mazda$media_v19
df[df$V3=='mazda' & is.na(df$V20),'V20'] = media_mazda$media_V20
#----------------------------------------------------------------------------------------
# Como temos apenas 2 carros da marca renault, vamos observar a distribuicao de V22 e V23
df[is.na(df$V22),]
df[df$V3=='renault',]
# Distribuicoes nao normais e outliers, vamos pela mediana
ggplot(df, aes(x=V22, y= ..density..)) +
geom_histogram(color='White', bins=10) +
geom_density()
ggplot(df, aes(x=V23, y= ..density..)) +
geom_histogram(color='White', bins=10) +
geom_density()
linhas <- !is.na(df$V22)
colunas <- c('V22', 'V23')
media_renault <- df[linhas,colunas] %>% summarise(mediana_v22=median(V22), mediana_V23=median(V23))
df[df$V3=='renault' & is.na(df$V22),'V22'] = media_renault$mediana_v22
df[df$V3=='renault' & is.na(df$V23),'V23'] = media_renault$mediana_V23
#----------------------------------------
# Verificacao dos veiculos com preco NA
df[is.na(df$V26),]
# Marcas para verificar 'audi', 'isuzu', 'porsche'
df[df$V3 %in% c('audi', 'isuzu', 'porsche'),]
#df[df$V3 %in% c('audi', 'isuzu', 'porsche') & !is.na(df$V26), c('V3', 'V26')]
#    %>% group_by(V3)
#    %>% summarise(max=max(V26), min=min(V26), mean=mean(V26), median=median(V26))
linhas <- df$V3 %in% c('audi', 'isuzu', 'porsche') & !is.na(df$V26)
colunas <- c('V3', 'V26')
median_marcas <- df[linhas, colunas] %>% group_by(V3) %>% summarise(median_26=median(V26))
for (marca in unique(median_marcas$V3)){
df[df$V3==marca & is.na(df$V26),'V26'] <- median_marcas[median_marcas$V3==marca,'median_26']
}
#----------------------
# Verificando coluna V2
teste <- df[df$V3 %in% unique(df[is.na(df$V2),'V3']),]
teste
# Para as marcas que possuem mais carros com V2 nao nulo, tiramos a media
linhas <- df$V3 %in% unique(df[is.na(df$V2),'V3']) & !is.na(df$V2)
colunas <- c('V3', 'V2')
media_marcas <- df[linhas, colunas] %>% group_by(V3) %>% summarise(median_2=median(V2))
media_marcas
for (marca in unique(media_marcas$V3) ){
df[df$V3==marca & is.na(df$V2),'V2'] <- media_marcas[media_marcas$V3==marca,'median_2']
}
# Para as marcas em que todos os carros apresentam V2 nulo, tiramos a mediana da base
ggplot(df, aes(x=V2, y= ..density..)) +
geom_histogram(color='White', bins=10) +
geom_density()
linhas <- !is.na(df$V2)
colunas <- 'V2'
mediana_marcas <- median(df[linhas, colunas])
mediana_marcas
for (marca in unique(df[is.na(df$V2), 'V3'])) {
df[df$V3==marca & is.na(df$V2),'V2'] <- mediana_marcas
}
# retirando duas linhas
# df <- na.omit(df, cols='V6')
# Validacao do conteudo de cada coluna para ver se tratamos todos os casos de [?]
for(col in colnames(df)){
print(col)
print(unique(df[,col]))
print('----------------------------')
}
# Construindo um gráfico com as distâncias intra-cluster
# colunas_porta = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "four")
# colunas_tracao = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "rwd", "fwd")
# colunas_cilindro = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "four", "six", "five", "three", "twelve", "two")
colunas_full = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "convertible", "hatchback", "sedan", "wagon", "four", "six", "five", "three", "twelve", "two")
#colunas = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "convertible", "hatchback", "sedan", "wagon")
colunas = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26")
numericas = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26")
# Normalizacao minmax das variaveis numericas
df_normalized <- min_max(df,numericas)
summary(df_normalized)
colnames(df_normalized)
# Seleção de atributos
# Iremos criar o onehot encoding para todas as features categoricas, assim podemos testar a sua eficiencia ao rodar os algoritmos de clusterizacao
# Criando features Categoricas via One Hot Encoding
df_normalized = onehot_features(df_normalized, 'V3')
df_normalized = onehot_features(df_normalized, 'V4')
df_normalized = onehot_features(df_normalized, 'V5')
# df_normalized = onehot_features(df_normalized, 'V6')
df_normalized = onehot_features(df_normalized, 'V7')
df_normalized = onehot_features(df_normalized, 'V8')
df_normalized = onehot_features(df_normalized, 'V9')
df_normalized = onehot_features(df_normalized, 'V15')
df_normalized = onehot_features(df_normalized, 'V16')
df_normalized = onehot_features(df_normalized, 'V18')
summary(df_normalized)
base_1 = df_normalized[, numericas]
fviz_nbclust(base_1, kmeans, method="wss",k.max=3)
fviz_nbclust(base_1, kmeans, method="wss",k.max=30)
# Aplica a tecnica K-Means
base_kmeans <- eclust(base_1, "kmeans", k=30,nstart=25, graph=TRUE)
fviz_nbclust(base_1, kmeans, method="wss",k.max=30)
# Construindo um gráfico com os valores da silhueta
fviz_nbclust(base_1, kmeans, method="silhouette", k.max=30)
melt_train_set <- df[, numericas]
melt_train_set <- melt(melt_train_set)
p <- ggplot(data=melt_train_set, aes(x=value))+
stat_density()+
facet_wrap(~variable, scales='free'); p
base_1 = df_normalized[, numericas_sem_21]
numericas_sem_21 = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V22", "V23", "V24", "V25", "V26")
base_1 = df_normalized[, numericas_sem_21]
fviz_nbclust(base_1, kmeans, method="wss",k.max=30)
base_1 = df_normalized[, numericas]
fviz_nbclust(base_1, kmeans, method="wss",k.max=30)
base_1 = df_normalized[, numericas_sem_21]
# Construindo um gráfico com os valores da silhueta
fviz_nbclust(base_1, kmeans, method="silhouette", k.max=30)
p <- ggplot(data=melt_train_set, aes(x=value))+
stat_density()+
facet_wrap(~variable, scales='free'); p
numericas_reduzido = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V23", "V24", "V25", "V26")
base_1 = df_normalized[, numericas_reduzido]
fviz_nbclust(base_1, kmeans, method="wss",k.max=30)
# Construindo um gráfico com os valores da silhueta
fviz_nbclust(base_1, kmeans, method="silhouette", k.max=30)
summary(base_1)
p <- ggplot(data=melt_train_set, aes(x=value))+
stat_density()+
facet_wrap(~variable, scales='free'); p
numericas_reduzido = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V20", "V23", "V24", "V25", "V26")
base_1 = df_normalized[, numericas_reduzido]
fviz_nbclust(base_1, kmeans, method="wss",k.max=30)
# Construindo um gráfico com os valores da silhueta
fviz_nbclust(base_1, kmeans, method="silhouette", k.max=30)
nb <- NbClust ( base_1 , distance ="euclidean",min.nc =2 , max.nc =30 , method ="complete",index ="all")
#TESTE KMEANS COM 3 para análise de grupos
base_kmeans_test <- kmeans(base_1 , 2, nstart = 25)
test_df <- as.data.frame.matrix(table(base_kmeans_test$cluster, df_normalized$V1));test_df
View(test_df)
p <- ggplot(data=melt_train_set, aes(x=value))+
stat_density()+
facet_wrap(~variable, scales='free'); p
numericas_reduzido = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V23", "V24", "V25", "V26")
fviz_nbclust(base_1, kmeans, method="wss",k.max=30)
base_1 = df_normalized[, numericas_reduzido]
fviz_nbclust(base_1, kmeans, method="wss",k.max=30)
# Construindo um gráfico com os valores da silhueta
fviz_nbclust(base_1, kmeans, method="silhouette", k.max=30)
# Aplica a tecnica K-Means
base_kmeans <- eclust(base_1, "kmeans", k=30,nstart=25, graph=TRUE)
# Aplica a tecnica K-Means
base_kmeans <- eclust(base_1, "kmeans", k=2,nstart=25, graph=TRUE)
#TESTE KMEANS COM 3 para análise de grupos
base_kmeans_test <- kmeans(base_1 , 2, nstart = 25)
test_df <- as.data.frame.matrix(table(base_kmeans_test$cluster, df_normalized$V1));test_df
#TESTE KMEANS COM 3 para análise de grupos
base_kmeans_test <- kmeans(base_1 , 4, nstart = 25)
test_df <- as.data.frame.matrix(table(base_kmeans_test$cluster, df_normalized$V1));test_df
p <- ggplot(data=melt_train_set, aes(x=value))+
stat_density()+
facet_wrap(~variable, scales='free'); p
numericas_reduzido = c("V2", "V10", "V11", "V12", "V13", "V14", "V19", "V23", "V24", "V25", "V26")
base_1 = df_normalized[, numericas_reduzido]
fviz_nbclust(base_1, kmeans, method="wss",k.max=30)
# Construindo um gráfico com os valores da silhueta
fviz_nbclust(base_1, kmeans, method="silhouette", k.max=30)
nb <- NbClust ( base_1 , distance ="euclidean",min.nc =2 , max.nc =30 , method ="complete",index ="all")
nb <- NbClust ( base_1 , distance ="manhattan",min.nc =2 , max.nc =30 , method ="complete",index ="all")
nb <- NbClust ( base_1 , distance ="euclidian",min.nc =2 , max.nc =30 , method ="complete",index ="all")
knitr::opts_chunk$set(echo = TRUE, error = FALSE, message = FALSE, warning = FALSE, tidy = FALSE)
options(digits = 3)
# Adicione os pacotes usados neste trabalho:
#install packages
#install.packages("apcluster")
#install.packages("fclust")
#install.packages("ppclust")
#install.packages("dbscan")
#install.packages("gridExtra")
#install.packages("fpc")
#install.packages("NbClust")
#install.packages("factoextra")
#install.packages("NbClust")
library(dplyr)
library(ggplot2)
library(factoextra)
library(NbClust)
library(fpc)
library(gridExtra)
library(dbscan)
library(ppclust)
library(fclust)
library(apcluster)
library ( NbClust )
library(reshape2)
#Funções extras
onehot_features <- function(dataset, feature){
cats <- unique(dataset[, feature])
for (cat in cats){
dataset[cat] <- as.numeric(dataset[,feature]==cat)
}
return(dataset)
}
min_max <- function(df, columns){
min_features <- apply(df[,colnames(df) %in% columns], 2, min); min_features
max_features <- apply(df[,colnames(df) %in% columns], 2, max); max_features
diff <- max_features - min_features; diff
df[,colnames(df) %in% columns] <- sweep(df[,colnames(df) %in% columns], 2, min_features, "-")
df[,colnames(df) %in% columns] <- sweep(df[,colnames(df) %in% columns], 2, diff, "/")
return(df)
}
normalization <- function(df, columns){
mean_features <- apply(df[,colnames(df) %in% columns], 2, mean);mean_features
sd_features <- apply(df[,colnames(df) %in% columns], 2, sd); sd_features
df[,colnames(df) %in% columns] <- sweep(df[,colnames(df) %in% columns], 2, mean_features, "-")
df[,colnames(df) %in% columns] <- sweep(df[,colnames(df) %in% columns], 2, sd_features, "/")
return(df)
}
#trainSet[,2:ncol(trainSet)] <- sweep(trainSet[,2:ncol(trainSet)], 2, mean_features, "-")
#trainSet[,2:ncol(trainSet)] <- sweep(trainSet[,2:ncol(trainSet)], 2, sd_features, "/")
# Configure ambiente de trabalho na mesma pasta
# onde colocou a base de dados:
# setwd("")
# setwd("C:\\Users\\Eric\\Documents\\GitHub\\mineiracao_dados_complexos\\Aprendizado de Maquina Nao Supervisionado\\Trabalho 3")
# setwd("/Users/nkuros/Documents/mineiracao_dados_complexos/Aprendizado de Maquina Nao Supervisionado/Trabalho 3/")
setwd("~/GitHub/mineiracao_dados_complexos/Aprendizado de Maquina Nao Supervisionado/Trabalho 3")
# Leitura da base
df <- read.table('imports-85.data',sep=',')
set.seed (12345)
# Tratamento de dados faltantes
# Avaliacao do dataset
summary(df)
head(df)
# Avalaiacao dos valores unicos de cada coluna
for(col in colnames(df)){
print(col)
print(unique(df[,col]))
print('\n')
}
#------------------------------------------------------------------------
# Tratando "?" que apresenta nas bases para poder transformar em numerico
df$V2 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V2))
df$V6 <- gsub(pattern='[?]', replacement=NA, df$V6)
df$V19 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V19))
df$V20 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V20))
df$V22 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V22))
df$V23 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V23))
df$V26 <- as.numeric(gsub(pattern='[?]', replacement=NA, df$V26))
unique(df$V6)
unique(df$V18)
#----------------------------------------
# Data set de teste retirando todos os NA
df2 <- na.omit(df)
summary(df2)
#----------------------------------------
# NA de V19 e V20 sao para a mesma marca mazda
df[is.na(df$V19),]
df[df$V3=='mazda',]
# Agrupando os valores para obter a m[edia de V19 e V20]
linhas = df$V3=='mazda' & !is.na(df$V19)
colunas = c('V3', 'V19', 'V20')
media_mazda = df[linhas,colunas] %>% group_by(V3) %>% summarise(media_v19=median(V19), media_V20=median(V20))
# Substituindo V19 e V20 que estao NA pela media
df[df$V3=='mazda' & is.na(df$V19),'V19'] = media_mazda$media_v19
df[df$V3=='mazda' & is.na(df$V20),'V20'] = media_mazda$media_V20
#----------------------------------------------------------------------------------------
# Como temos apenas 2 carros da marca renault, vamos observar a distribuicao de V22 e V23
df[is.na(df$V22),]
df[df$V3=='renault',]
# Distribuicoes nao normais e outliers, vamos pela mediana
ggplot(df, aes(x=V22, y= ..density..)) +
geom_histogram(color='White', bins=10) +
geom_density()
ggplot(df, aes(x=V23, y= ..density..)) +
geom_histogram(color='White', bins=10) +
geom_density()
linhas <- !is.na(df$V22)
colunas <- c('V22', 'V23')
media_renault <- df[linhas,colunas] %>% summarise(mediana_v22=median(V22), mediana_V23=median(V23))
df[df$V3=='renault' & is.na(df$V22),'V22'] = media_renault$mediana_v22
df[df$V3=='renault' & is.na(df$V23),'V23'] = media_renault$mediana_V23
#----------------------------------------
# Verificacao dos veiculos com preco NA
df[is.na(df$V26),]
# Marcas para verificar 'audi', 'isuzu', 'porsche'
df[df$V3 %in% c('audi', 'isuzu', 'porsche'),]
#df[df$V3 %in% c('audi', 'isuzu', 'porsche') & !is.na(df$V26), c('V3', 'V26')]
#    %>% group_by(V3)
#    %>% summarise(max=max(V26), min=min(V26), mean=mean(V26), median=median(V26))
linhas <- df$V3 %in% c('audi', 'isuzu', 'porsche') & !is.na(df$V26)
colunas <- c('V3', 'V26')
median_marcas <- df[linhas, colunas] %>% group_by(V3) %>% summarise(median_26=median(V26))
for (marca in unique(median_marcas$V3)){
df[df$V3==marca & is.na(df$V26),'V26'] <- median_marcas[median_marcas$V3==marca,'median_26']
}
#----------------------
# Verificando coluna V2
teste <- df[df$V3 %in% unique(df[is.na(df$V2),'V3']),]
teste
# Para as marcas que possuem mais carros com V2 nao nulo, tiramos a media
linhas <- df$V3 %in% unique(df[is.na(df$V2),'V3']) & !is.na(df$V2)
colunas <- c('V3', 'V2')
media_marcas <- df[linhas, colunas] %>% group_by(V3) %>% summarise(median_2=median(V2))
media_marcas
for (marca in unique(media_marcas$V3) ){
df[df$V3==marca & is.na(df$V2),'V2'] <- media_marcas[media_marcas$V3==marca,'median_2']
}
# Para as marcas em que todos os carros apresentam V2 nulo, tiramos a mediana da base
ggplot(df, aes(x=V2, y= ..density..)) +
geom_histogram(color='White', bins=10) +
geom_density()
linhas <- !is.na(df$V2)
colunas <- 'V2'
mediana_marcas <- median(df[linhas, colunas])
mediana_marcas
for (marca in unique(df[is.na(df$V2), 'V3'])) {
df[df$V3==marca & is.na(df$V2),'V2'] <- mediana_marcas
}
# retirando duas linhas
# df <- na.omit(df, cols='V6')
# Validacao do conteudo de cada coluna para ver se tratamos todos os casos de [?]
for(col in colnames(df)){
print(col)
print(unique(df[,col]))
print('----------------------------')
}
# Construindo um gráfico com as distâncias intra-cluster
# colunas_porta = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "four")
# colunas_tracao = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "rwd", "fwd")
# colunas_cilindro = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "four", "six", "five", "three", "twelve", "two")
colunas_full = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "convertible", "hatchback", "sedan", "wagon", "four", "six", "five", "three", "twelve", "two")
#colunas = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26", "convertible", "hatchback", "sedan", "wagon")
colunas = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26")
numericas = c("V2", "V10", "V11", "V12", "V13", "V14", "V17", "V19", "V20", "V21", "V22", "V23", "V24", "V25", "V26")
# Normalizacao minmax das variaveis numericas
df_normalized <- min_max(df,numericas)
# df_normalized <- normalization(df, numericas)
summary(df_normalized)
colnames(df_normalized)
# Seleção de atributos
# Iremos criar o onehot encoding para todas as features categoricas, assim podemos testar a sua eficiencia ao rodar os algoritmos de clusterizacao
# Criando features Categoricas via One Hot Encoding
# df_normalized = onehot_features(df_normalized, 'V3')
df_normalized = onehot_features(df_normalized, 'V4')
df_normalized = onehot_features(df_normalized, 'V5')
# df_normalized = onehot_features(df_normalized, 'V6')
df_normalized = onehot_features(df_normalized, 'V7')
df_normalized = onehot_features(df_normalized, 'V8')
df_normalized = onehot_features(df_normalized, 'V9')
df_normalized = onehot_features(df_normalized, 'V15')
df_normalized = onehot_features(df_normalized, 'V16')
df_normalized = onehot_features(df_normalized, 'V18')
melt_train_set <- df[, numericas]
melt_train_set <- melt(melt_train_set)
p <- ggplot(data=melt_train_set, aes(x=value))+
stat_density()+
facet_wrap(~variable, scales='free'); p
numericas_reduzido <- c("V2", "V10", "V11", "V12", "V13", "V14", "V19", "V23", "V24", "V25", "V26")
summary(df_normalized)
c(1, 2) + c(3, 4)
summary(df)
df_normalized
colnames(df_normalized)
pc <- prcomp(df[, numericas_reduzido], center = TRUE,scale. = TRUE)
en <- cumsum(pc$sdev / sum(pc$sdev))
en
pc
z <- pc$x[, 1:7]
z
?scale
nb <- NbClust(z, distance="euclidean", min.nc=2, max.nc=30, method="complete", index="all")
nb <- NbClust(z, distance="euclidean", min.nc=2, max.nc=30, method="complete", index="all")
fviz_nbclust(nb) + theme_minimal()
colnames(df_normalized)
categoricas_reduzido <- c("gas", "turbo", "convertible", "hatchback", "sedan", "wagon", "four", "six", "five", "three", "twelve", "two")
pc <- prcomp(df[, c(numericas_reduzido,categoricas_reduzido)], center = TRUE,scale. = TRUE)
pc <- prcomp(df_normalized[, c(numericas_reduzido)], center = TRUE,scale. = TRUE)
en <- cumsum(pc$sdev / sum(pc$sdev))
en
z <- pc$x[, 1:7]
# z <- scale(z)
z <- min_max(z)
en
z
# z <- scale(z)
z <- min_max(z)
# z <- scale(z)
z <- min_max(z, colnames(z))
nb <- NbClust(z, distance="euclidean", min.nc=2, max.nc=30, method="complete", index="all")
nb <- NbClust(z, distance="euclidean", min.nc=2, max.nc=30, method="complete", index="all")
fviz_nbclust(nb) + theme_minimal()
categoricas_reduzido <- c("gas", "turbo", "convertible", "hatchback", "sedan", "wagon", "four", "six", "five", "three", "twelve", "two")
pc <- prcomp(df_normalized[, c(numericas_reduzido,categoricas_reduzido)], center = TRUE,scale. = TRUE)
en <- cumsum(pc$sdev / sum(pc$sdev))
en
z <- pc$x[, 1:7]
z <- pc$x[, 1:16]
# z <- scale(z)
z <- min_max(z, colnames(z))
nb <- NbClust(z, distance="euclidean", min.nc=2, max.nc=30, method="complete", index="all")
nb <- NbClust(z, distance="euclidean", min.nc=2, max.nc=30, method="complete", index="all")
fviz_nbclust(nb) + theme_minimal()
df_normalized <- scale(df[, numericas])
df_normalized <- cbind(df_normalized, df[,-(colnames(df) %in% numericas) ])
summary(df_normalized)
df_normalized <- scale(df[, numericas])
summary(df_normalized)
colnames( df[,-(colnames(df) %in% numericas) ])
colnames(df[,-c(numericas) ])
colnames(df[,-numericas])
numericas
df_normalized <- cbind(df_normalized, df[,(colnames(df) %notin% numericas) ])
`%notin%` <- Negate(`%in%`)
df_normalized <- cbind(df_normalized, df[,(colnames(df) %notin% numericas) ])
colnames(df_normalized)
df_normalized = onehot_features(df_normalized, 'V15')
# Criando features Categoricas via One Hot Encoding
# df_normalized = onehot_features(df_normalized, 'V3')
df_normalized = onehot_features(df_normalized, 'V4')
df_normalized = onehot_features(df_normalized, 'V5')
# df_normalized = onehot_features(df_normalized, 'V6')
df_normalized = onehot_features(df_normalized, 'V7')
df_normalized = onehot_features(df_normalized, 'V8')
df_normalized = onehot_features(df_normalized, 'V9')
df_normalized = onehot_features(df_normalized, 'V16')
df_normalized = onehot_features(df_normalized, 'V18')
